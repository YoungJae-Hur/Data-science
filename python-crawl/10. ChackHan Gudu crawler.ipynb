{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = 'https://xn--2e0bs4kirwfni.kr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Crawl category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_data(url):\n",
    "    headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36'\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    category_el = soup.select('#JD-Header > .gnb > ul > li.xans-record-', href=True)\n",
    "    \n",
    "    categories = []\n",
    "    for i in category_el:\n",
    "        url = i.find('a', href=True)['href']\n",
    "        if 'http' not in url:\n",
    "            url = baseUrl + url\n",
    "        categories.append({i.text: url})\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " categoriesMap = get_category_data('https://xn--2e0bs4kirwfni.kr/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Crawl product name per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_name(url):\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36'\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    \n",
    "    product_el = soup.select('.xans-element-.xans-product.xans-product-normalpackage.list_normal ul.list_common > li')\n",
    "    \n",
    "    products = []\n",
    "    \n",
    "    for i in product_el:\n",
    "        soup.find('b', {'class':'hdd_txt'}).decompose()\n",
    "        productName = i.find('div', {'class':'opt_set'}).find('span')(text=True)\n",
    "        producturl = i.find('p', {'class':'tit'}).find('a', href=True)['href']\n",
    "        if 'http' not in producturl:\n",
    "            producturl = baseUrl + producturl\n",
    "        products.append([productName[0], producturl])\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productMap = {}\n",
    "for i in categoriesMap:\n",
    "    url = list(i.values())[0]\n",
    "    products = get_product_name(url)\n",
    "    productMap[list(i.keys())[0]] = products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. crawl review data for each product in category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_review_data(url):\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    category_el = soup.select('.xans-product-review .ec-base-table table tbody tr')\n",
    "    driver.close()\n",
    "    \n",
    "    reviews = []\n",
    "    for i, el in enumerate(category_el, start=1):\n",
    "        if (i % 2 == 1): #odd\n",
    "            postNum = el.findAll('td')[0](text=True)[0]\n",
    "            title = el.findAll('td')[1](text=True)[1]\n",
    "            reviewer = el.findAll('td')[2](text=True)[0]\n",
    "            postDate = el.findAll('td')[3](text=True)[0]\n",
    "            score = el.findAll('td')[5].find('img')['alt'].split('점')[0]\n",
    "            continue\n",
    "        else:            # even\n",
    "            text = el.find('p', {'class':'word'})\n",
    "            if text is None:\n",
    "                text = el.find('p').get_text()\n",
    "            else:\n",
    "                text = text.text\n",
    "            imgUrls = el.find('div', {'class':'view'}).findAll('img')\n",
    "\n",
    "            imgUrlList = []\n",
    "            for img in imgUrls:\n",
    "                imgurl = 'https:' + img['src']\n",
    "                imgUrlList.append(imgurl)\n",
    "            reviews.append([\n",
    "                postNum,\n",
    "                title,\n",
    "                reviewer,\n",
    "                postDate,\n",
    "                score,\n",
    "                text,\n",
    "                imgUrlList,\n",
    "            ])\n",
    "    return reviews\n",
    "    \n",
    "    # method 1 - using normal request w/o selenium, its fast but not accurate eg. no text review content \n",
    "#     resp = requests.get(url, headers=headers)\n",
    "#     soup = BeautifulSoup(resp.text)\n",
    "#     category_el = soup.select('.xans-product-review .ec-base-table table tbody tr')\n",
    "#     print('length of category_el: ', len(category_el))\n",
    "\n",
    "#     categories = []\n",
    "#     for i in category_el:\n",
    "#         row = i.findAll('td')[0](text=True)\n",
    "#         print('1. ', row[0])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in productMap.keys():\n",
    "    \n",
    "    print(category)\n",
    "    \n",
    "    for l in productMap[category]:\n",
    "        productName = l[0]\n",
    "        productUrl = l[1]\n",
    "        print(productName, productUrl)\n",
    "        # extract_review_data(productUrl) # not stable function, this is for extracting all review data\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a category = '쿨리어런스' case\n",
    "category = '쿨리어런스'\n",
    "product = productMap['쿨리어런스'][0][0]\n",
    "product_detail_url = productMap['쿨리어런스'][0][1]\n",
    "review_data = extract_review_data('https://xn--2e0bs4kirwfni.kr/product/벤티-스트랩-샌들-se-sdacs2c085/2285/category/137/display/1/')\n",
    "\n",
    "file_data = []\n",
    "\n",
    "for rd in review_data:\n",
    "    file_data.append([\n",
    "        category,\n",
    "        product,\n",
    "        product_detail_url,\n",
    "        rd[0],\n",
    "        rd[1],\n",
    "        rd[2],\n",
    "        rd[3],\n",
    "        rd[4],\n",
    "        rd[5],\n",
    "        rd[6],\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert list to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_data, columns=['Category','Product','Product_detail_url','PostNum','Title','Reviewer', 'Date', 'Score', 'Text', 'ImageUrls'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. save filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ChackHan_product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_review_data('https://xn--2e0bs4kirwfni.kr/product/벤티-스트랩-샌들-se-sdacs2c085/2285/category/137/display/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_review_data('https://xn--2e0bs4kirwfni.kr/product/%EC%86%94%EB%A0%88%EC%9D%B4-%ED%94%8C%EB%9E%AB-%EB%AE%AC-%EC%83%8C%EB%93%A4-ne-spacr2b3091/1852/category/137/display/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_review_data('https://xn--2e0bs4kirwfni.kr/product/%EC%98%A4%EC%8A%AC%EB%A1%9C-%ED%88%AC%EC%9B%A8%EC%9D%B4-%EB%9D%BC%ED%83%84-%EC%83%8C%EB%93%A4-se-sdltr2c129/2317/category/137/display/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
